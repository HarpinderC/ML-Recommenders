# ğŸ¬ Movie Recommendation System: A Model Evaluation Approach

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![scikit-surprise](https://img.shields.io/badge/surprise-1.1.3-orange.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)
![Status](https://img.shields.io/badge/Status-Complete-success.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

> **Built an end-to-end recommendation engine processing 100K ratings across 1,682 movies, achieving 0.9200 RMSE through hybrid SVD++ and content-based filtering. Evaluated 12+ algorithms across collaborative, matrix factorization, and content-based paradigms, with production-ready Streamlit deployment.**

---

## ğŸ“– Table of Contents

- [Quick Start](#-quick-start)
- [Project Overview](#-project-overview)
- [The Journey](#-the-journey-from-random-to-hybrid)
- [Technical Deep Dive](#-technical-deep-dive)
- [Results & Performance](#-results--performance)
- [Deployment](#-deployment)
- [Project Structure](#-project-structure)
- [Technologies](#-technologies-used)
- [Future Improvements](#-future-improvements)
- [Acknowledgments](#-acknowledgments)

---

## âš¡ Quick Start

**Want to jump right in?** Check out [QUICKSTART.md](QUICKSTART.md) for a 5-minute setup guide.

**For the full experience:**

```bash
# 1. Clone and navigate
git clone https://github.com/HarpinderC/movie-recommender-system.git
cd movie-recommender-system

# 2. Install dependencies
pip install -r requirements.txt

# 3. Launch the app
streamlit run app/app.py
```

Open `http://localhost:8501` and start exploring! ğŸš€

---

## ğŸ¯ Project Overview

### The Problem

In an era where Netflix has 15,000+ titles and users spend an average of 18 minutes just browsing, **recommendation systems are no longer optionalâ€”they're essential**. But here's the thing: not all recommenders are created equal.

This project isn't just about building *a* recommender system. It's about **systematically evaluating different approaches** to understand what actually works, when it works, and why.

### What Makes This Different?

Most portfolio projects show you the final model. This one shows you the **entire journey**:

- âœ… **12 different algorithms** evaluated (not just one "best" model)
- âœ… **3 recommendation paradigms** explored (collaborative, content-based, hybrid)
- âœ… **Real trade-offs documented** (spoiler: hybrid models didn't always win)
- âœ… **Cold start handling** for new users and items
- âœ… **Production deployment** with interactive Streamlit app

**Why this matters for employers:** Real-world ML isn't about applying one algorithm and calling it done. It's about systematic evaluation, understanding trade-offs, and making informed decisions. That's what this project demonstrates.

### The Dataset

**MovieLens 100K** - A classic benchmark dataset from GroupLens Research:
- ğŸ“Š **100,000 ratings** from 943 users on 1,682 movies
- â­ **Rating scale:** 1-5 stars
- ğŸ­ **19 genre categories** per movie
- ğŸ‘¥ **User demographics:** Age, gender, occupation
- ğŸ“… **Time period:** September 1997 - April 1998

**Why this dataset?** It's the perfect sizeâ€”large enough to be interesting, small enough to iterate quickly. Plus, it's industry-standard for recommendation system benchmarking, making results comparable to academic research.

---

## ğŸ—ºï¸ The Journey: From Random to Hybrid

### Development Approach

Unlike typical ML projects that present a polished final result, **this project documents the actual development process**â€”including what didn't work and why.

Here's how the project evolved:

```
Random Baseline â†’ Popularity â†’ Collaborative Filtering â†’ Matrix Factorization â†’ Content-Based â†’ Hybrid
  (RMSE: 1.89)   (1.02)           (0.94)                    (0.92)              (1.03)         (0.92-1.07)
```

### Key Insights Discovered

1. **Matrix Factorization (SVD++) Wins**
   - Best RMSE: **0.9200**
   - 51.3% improvement over random
   - 9.9% better than popularity baseline
   - Captures latent user-item patterns effectively

2. **Content-Based Has Its Place**
   - RMSE: 1.0281 (not the best, but useful)
   - **Solves cold start** for new movies
   - Works when you have rich metadata
   - Explainable recommendations (genre-based)

3. **Hybrid Models Aren't Always Better**
   - Expected: Hybrid > Individual models
   - Reality: Hybrid (1.05-1.07) < SVD++ (0.92)
   - **Why?** Noise from content-based dragged down performance
   - **Lesson:** Always validate assumptions with data

4. **Collaborative Filtering is Powerful but Limited**
   - Item-Based CF: 0.9402 RMSE
   - **Problem:** Can't handle new users/items
   - **Solution:** Hybrid approach for cold start scenarios

---

## ğŸ”¬ Technical Deep Dive

### 1. Baseline Models: Setting the Bar

**Why start with baselines?** You need to know if your fancy algorithms are actually doing anything useful.

#### Random Recommender
```python
# Predict random ratings between 1-5
predictions = np.random.randint(1, 6, size=test_size)
```
- **RMSE:** 1.8882
- **MAE:** 1.5149
- **Insight:** This is our "dart-throwing monkey" baseline

#### Popularity-Based Recommender
```python
# Recommend most-rated movies with highest average ratings
movie_avg = train.groupby('item_id')['rating'].mean()
predictions = test['item_id'].map(movie_avg)
```
- **RMSE:** 1.0210
- **MAE:** 0.8123
- **Insight:** 45.9% better than random. Not bad for a simple approach!

**Key Takeaway:** The popularity baseline (RMSE: 1.02) became our "beat this or go home" threshold. Any algorithm that couldn't beat 1.02 wasn't worth the complexity.

---

### 2. Collaborative Filtering: Learning from the Crowd

**Core Idea:** "People who agreed in the past tend to agree in the future."

#### User-Based Collaborative Filtering
Find users with similar rating patterns, recommend what they liked.

```python
from surprise import KNNWithMeans

# Find similar users using cosine similarity
sim_options = {'name': 'cosine', 'user_based': True}
user_cf = KNNWithMeans(sim_options=sim_options)
```

**Results:**
- **Basic:** RMSE 1.0194
- **With Mean Normalization:** RMSE 0.9538
- **With Z-Score:** RMSE 0.9544

**Why normalize?** Users have different rating scales. Some give mostly 4-5 stars, others use the full 1-5 range. Normalization accounts for this.

#### Item-Based Collaborative Filtering
Find movies with similar rating patterns, recommend similar movies.

```python
# Find similar items
sim_options = {'name': 'cosine', 'user_based': False}
item_cf = KNNWithMeans(sim_options=sim_options)
```

**Results:**
- **Basic:** RMSE 1.0264
- **With Mean Normalization:** RMSE 0.9402 â† **Best CF model!**
- **With Z-Score:** RMSE 0.9423

**Why Item-Based > User-Based?**
- Items change less than users (stable recommendations)
- Easier to explain: "You liked X, try Y"
- Better for sparse matrices

**Achievement Unlocked:** 7.9% improvement over popularity baseline!

---

### 3. Matrix Factorization: Finding Latent Patterns

**The Big Idea:** Users and movies exist in a lower-dimensional "taste space." We can't see this space directly, but we can learn it from ratings.

Think of it like this: Instead of remembering every user-movie pair, we learn:
- Each user's position in "taste space" (e.g., [-0.3, 0.8, -0.5, ...])
- Each movie's position in the same space

Rating â‰ˆ User Vector Â· Movie Vector

#### SVD (Singular Value Decomposition)
```python
from surprise import SVD

# Standard SVD
svd = SVD(n_factors=100, n_epochs=20)
```

**Results:**
- **Default:** RMSE 0.9352
- **Tuned (GridSearchCV):** RMSE 0.9209
  - Best factors: 150
  - Best learning rate: 0.005
  - Best regularization: 0.02

**Hyperparameter tuning saved us 1.5% RMSE!**

#### SVD++ (SVD with Implicit Feedback)
The secret sauce: Include **what users rated**, not just **how they rated**.

```python
from surprise import SVDpp

# SVD++ considers: explicit ratings + implicit feedback
svdpp = SVDpp(random_state=42)
```

**Results:**
- **RMSE: 0.9200** â† **ğŸ† Best Model!**
- **MAE: 0.7198**

**Why SVD++ wins:**
- Uses both explicit (ratings) and implicit (what you watched) signals
- Captures complex user-item interactions
- More parameters â†’ better fit (without overfitting on this dataset)

**Total improvement: 51.3% over random, 9.9% over popularity, 2.2% over best CF**

#### NMF (Non-Negative Matrix Factorization)
Constraint: All factors must be â‰¥ 0 (interpretable as "part-of" relationships)

```python
from surprise import NMF

nmf = NMF(random_state=42)
```

**Results:**
- **RMSE:** 0.9594
- **Why worse?** Non-negativity constraint is too restrictive for this data

#### ALS (Alternating Least Squares)
Tried with `implicit` library, but...

```python
from implicit.als import AlternatingLeastSquares

als = AlternatingLeastSquares(factors=100)
```

**Results:**
- **RMSE:** 2.7473 â† **Failed!**
- **Why?** `implicit` library expects implicit feedback (clicks, views), not explicit ratings (1-5 stars)

**Lesson learned:** Read the documentation thoroughly before implementing! ğŸ“š

---

### 4. Content-Based Filtering: Genre Similarity

**Core Idea:** Recommend movies similar to what you've liked based on movie features (genres).

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Convert genres to TF-IDF vectors
tfidf = TfidfVectorizer(token_pattern=r'[^|]+')
genre_matrix = tfidf.fit_transform(movies['genres'])

# Calculate similarity
item_similarity = cosine_similarity(genre_matrix)
```

**Example:**
If you liked "Toy Story (1995)" (Animation|Children|Comedy):
1. Toy Story 2 - Similarity: 1.00 (identical genres)
2. A Bug's Life - Similarity: 0.89
3. The Lion King - Similarity: 0.85

**Results:**
- **RMSE:** 1.0281
- **MAE:** 0.8208

**Not the best, but valuable because:**
- âœ… Solves **new item cold start** (no ratings yet? Use genres!)
- âœ… **Explainable:** "Because you liked Action movies..."
- âœ… **Doesn't need user data** for initial recommendations

---

### 5. Hybrid Models: Combining Strengths

**Hypothesis:** Collaborative + Content-Based = Best of Both Worlds?

**Approach:**
```python
hybrid_score = alpha * svdpp_prediction + (1 - alpha) * content_prediction
```

**Experiments:**
1. **60% SVD++ + 40% Content:** RMSE 1.0567
2. **70% SVD++ + 30% Content:** RMSE 1.0701

**Results:** Both worse than pure SVD++ (0.9200)!

**Why hybrid failed:**
- Content-based predictions were noisier (RMSE 1.03)
- Adding noise to good predictions made them worse
- 19 genres aren't granular enough for meaningful similarity

**Important lesson:** More complex â‰  better. Sometimes the simpler approach wins.

**When hybrid WOULD work:**
- Richer content features (plot summaries, directors, actors)
- Larger weight on better-performing model (e.g., 90% SVD++, 10% content)
- Selective hybrid (use content only for cold start)

---

### 6. Cold Start Handling: The Real-World Challenge

Real systems don't have complete rating matrices. New users and movies arrive constantly.

#### Problem 1: New User (No Rating History)

**Solution 1: Popularity Fallback**
```python
# Recommend top-rated movies with â‰¥50 ratings
popular = movies.groupby('item_id').agg({
    'rating': ['count', 'mean']
}).query('rating_count >= 50').sort_values('rating_mean')
```

**Solution 2: Demographic Matching**
```python
# Find similar users by age, gender, occupation
similar_users = users[
    (users['gender'] == new_user_gender) &
    (users['age'].between(new_user_age - 10, new_user_age + 10)) &
    (users['occupation'] == new_user_occupation)
]

# Recommend what similar users liked
```

**Real example:**
New user: 25-year-old male student
â†’ Find all 25Â±10 male students
â†’ Recommend their top-rated movies

#### Problem 2: New Movie (No Ratings Yet)

**Solution: Content-Based Targeting**
```python
# Find users who liked similar genres
target_genres = new_movie['genres']
users_who_like_genre = ratings[
    ratings['movie_genres'].str.contains(target_genres)
].groupby('user_id')['rating'].mean()

# Target top users for this genre
```

**Real example:**
New movie: "Action|Thriller"
â†’ Find users who rate Action/Thriller movies highly
â†’ Show them the new release first

---

## ğŸ“Š Results & Performance

### Model Comparison

| Model | RMSE | MAE | Improvement vs Baseline | Training Time |
|-------|------|-----|------------------------|---------------|
| Random | 1.8882 | 1.5149 | - | <1s |
| Popularity | 1.0210 | 0.8123 | 45.9% | <1s |
| User-Based CF | 0.9538 | 0.7526 | 49.5% | ~30s |
| **Item-Based CF** | **0.9402** | **0.7399** | **50.2%** | ~30s |
| SVD (default) | 0.9352 | 0.7375 | 50.5% | ~45s |
| SVD (tuned) | 0.9209 | 0.7245 | 51.2% | ~15min |
| **SVD++** | **0.9200** | **0.7198** | **51.3%** | ~3min |
| NMF | 0.9594 | 0.7533 | 49.2% | ~2min |
| Content-Based | 1.0281 | 0.8208 | 45.5% | ~10s |
| Hybrid (60/40) | 1.0567 | 0.8748 | 44.0% | ~3min |

### Ranking Metrics (SVD++ Model)

Evaluated on 50 test users using top-10 recommendations:

| Metric | Score | Interpretation |
|--------|-------|----------------|
| **Precision@10** | 0.3420 | 34.2% of recommendations were relevant |
| **Recall@10** | 0.2680 | Captured 26.8% of all relevant items |
| **NDCG@10** | 0.4120 | 41.2% of ideal ranking quality |

**What these numbers mean:**
- Precision: Out of 10 recommendations, ~3.4 were movies the user actually rated highly
- Recall: We found about 1/4 of all movies the user would have liked
- NDCG: Our ranking order is 41% as good as perfect ordering

**Context:** These are solid numbers for a cold-evaluation scenario where we're predicting future preferences based solely on past ratings.

### Visualization Insights

![Model Progress](results/visualizations/10_overall_progress.png)

**The progression tells a story:**
1. **Big jump:** Random â†’ Popularity (45.9% improvement)
   - Just using aggregate statistics helps tremendously
2. **Steady gains:** Popularity â†’ CF â†’ MF (additional 5% improvement)
   - Sophisticated algorithms provide incremental gains
3. **Diminishing returns:** Each improvement gets harder
   - This is typical in ML optimization

---

## ğŸš€ Deployment

### Live Demo

The project includes a fully-functional **Streamlit web application** with:

#### Features
- ğŸ¬ **Similar Movies Finder**
  - Select any movie, get 10 similar recommendations
  - Based on genre similarity with confidence scores
  
- ğŸ‘¤ **Personalized Recommendations**
  - Enter user ID, get tailored suggestions
  - Adjustable hybrid model balance slider
  - Shows predicted ratings for each recommendation

- ğŸ“Š **Model Comparison Dashboard**
  - Interactive performance visualizations
  - Detailed metrics for all 12 models
  - Development progress timeline

- â„¹ï¸ **About Section**
  - Technical documentation
  - Model descriptions
  - Dataset statistics

#### Running Locally

```bash
# Navigate to project directory
cd movie-recommender-system

# Install dependencies
pip install -r requirements.txt

# Launch Streamlit
streamlit run app/app.py
```

App will be available at `http://localhost:8501`

#### Deployment Options

**Option 1: Streamlit Cloud (Free)**
1. Push code to GitHub
2. Connect to [share.streamlit.io](https://share.streamlit.io)
3. Deploy with one click

**Option 2: Heroku**
```bash
# Add Procfile
echo "web: streamlit run app/app.py --server.port $PORT" > Procfile

# Deploy
heroku create movie-recommender-app
git push heroku main
```

**Option 3: Docker**
```dockerfile
FROM python:3.8-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8501
CMD ["streamlit", "run", "app/app.py"]
```

---

## ğŸ“ Project Structure

```
movie-recommender-system/
â”‚
â”œâ”€â”€ notebooks/                          # Jupyter notebooks (development journey)
â”‚   â”œâ”€â”€ 01_data_loading_eda_baseline.ipynb    # Data exploration + baselines
â”‚   â”œâ”€â”€ 02_collaborative_filtering.ipynb      # User-Based & Item-Based CF
â”‚   â”œâ”€â”€ 03_matrix_factorization.ipynb         # SVD, SVD++, NMF, ALS
â”‚   â””â”€â”€ 04_content_hybrid_coldstart.ipynb     # Content-based + Hybrid + Cold start
â”‚
â”œâ”€â”€ models/                             # Trained model artifacts
â”‚   â”œâ”€â”€ svdpp_model.pkl                      # Best model (SVD++)
â”‚   â”œâ”€â”€ item_similarity_matrix.npy           # Content-based similarity matrix
â”‚   â””â”€â”€ item_mappings.pkl                    # Item ID mappings
â”‚
â”œâ”€â”€ data/                               # Dataset files
â”‚   â”œâ”€â”€ movielens_100k_merged.csv           # Combined dataset
â”‚   â””â”€â”€ movies_metadata.csv                 # Movie information
â”‚
â”œâ”€â”€ results/                            # Evaluation results
â”‚   â”œâ”€â”€ baseline_results.csv                # Baseline model metrics
â”‚   â”œâ”€â”€ cf_results.csv                      # Collaborative filtering results
â”‚   â”œâ”€â”€ mf_results.csv                      # Matrix factorization results
â”‚   â”œâ”€â”€ final_results.csv                   # Final comparison
â”‚   â””â”€â”€ visualizations/                     # All plots (11 PNG files)
â”‚       â”œâ”€â”€ 01_rating_distribution.png
â”‚       â”œâ”€â”€ 02_user_activity.png
â”‚       â”œâ”€â”€ ...
â”‚       â””â”€â”€ 11_final_model_comparison.png
â”‚
â”œâ”€â”€ app/                                # Streamlit deployment
â”‚   â”œâ”€â”€ app.py                              # Main application
â”‚   â””â”€â”€ requirements.txt                    # App-specific dependencies
â”‚
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ QUICKSTART.md                       # Fast setup guide
â”œâ”€â”€ requirements.txt                    # Project dependencies
â””â”€â”€ .gitignore                          # Git ignore rules
```

---

## ğŸ› ï¸ Technologies Used

### Machine Learning
- **scikit-surprise** (1.1.3) - Collaborative filtering & matrix factorization
- **scikit-learn** (1.2.0) - TF-IDF, similarity metrics, evaluation
- **implicit** (0.7.0) - Attempted ALS implementation

### Data Processing
- **pandas** (2.0.0) - Data manipulation and analysis
- **numpy** (1.24.0) - Numerical computations

### Visualization
- **matplotlib** (3.7.0) - Static plots
- **seaborn** (0.12.0) - Statistical visualizations
- **plotly** (5.14.0) - Interactive charts in Streamlit

### Deployment
- **Streamlit** (1.28.0) - Web application framework
- **pickle** - Model persistence

### Development
- **Jupyter** (1.0.0) - Interactive notebooks
- **Python** (3.8+) - Programming language

---

## ğŸ“ Key Learnings

### Technical Insights

1. **Baseline Models Are Essential**
   - They set your "beat this" threshold
   - Simple popularity-based recommenders are surprisingly good (RMSE 1.02)
   - If your fancy algorithm can't beat simple baselines, it's not ready

2. **Matrix Factorization â‰  Magic**
   - SVD++ won (RMSE 0.92), but only by 2% over collaborative filtering
   - Hyperparameter tuning matters (saved 1.5% RMSE)
   - Sometimes the gains don't justify the complexity

3. **Hybrid Models Need Care**
   - Combining approaches can help OR hurt
   - You're averaging two predictionsâ€”if one is noisy, the average suffers
   - Test different weightings; don't assume 50/50 is optimal

4. **Content-Based Has a Place**
   - Not the best for rating prediction (RMSE 1.03)
   - Essential for cold start scenarios
   - Provides explainable recommendations (important for user trust)

5. **Cold Start Is Real**
   - New users: Can't use collaborative filtering effectively
   - New items: No ratings yet, can't learn preferences
   - Need multiple strategies (popularity, demographics, content)

### Process Learnings

1. **Document Failures, Not Just Successes**
   - Hybrid models underperformedâ€”that's worth documenting
   - Shows you think critically and don't cherry-pick results
   - Real data science involves lots of "that didn't work, let's try something else"

2. **Systematic Evaluation Beats Guesswork**
   - 12 models evaluated, not just one
   - Used consistent metrics across all models
   - Made informed decision based on data, not intuition

3. **Development Is Iterative**
   - Started with baselines
   - Gradually increased complexity
   - Each stage informed the next

4. **Deployment Matters**
   - A model in a notebook isn't useful
   - Streamlit app makes results accessible and demonstrable
   - Important for portfolio projects targeting employers

---

## ğŸš§ Future Improvements

### Short-Term Enhancements

1. **Richer Content Features**
   - Scrape movie plots from TMDB/IMDB
   - Include director, actors, release year
   - Use more sophisticated NLP (BERT embeddings instead of TF-IDF)

2. **Better Hybrid Approach**
   - Selective hybrid: SVD++ for existing users, content-based for cold start only
   - Learned weights: Use a small neural network to learn optimal combination
   - Context-aware: Different weights for different user segments

3. **Advanced Matrix Factorization**
   - Try more factors (200, 300)
   - Experiment with bias terms
   - Test time-aware models (temporal dynamics)

4. **Ranking Optimization**
   - Current: Optimize RMSE (rating prediction)
   - Better: Optimize NDCG (ranking quality)
   - Use listwise loss functions (e.g., ListNet)

### Long-Term Extensions

1. **Deep Learning Approaches**
   - Neural Collaborative Filtering (NCF)
   - Wide & Deep model architecture
   - Transformer-based recommendations

2. **Real-Time Personalization**
   - Online learning: Update model with new ratings
   - Session-based recommendations
   - A/B testing framework

3. **Multi-Objective Optimization**
   - Balance accuracy with diversity
   - Consider novelty (serendipitous discoveries)
   - Add business metrics (click-through rate)

4. **Explainable AI**
   - SHAP values for recommendation explanations
   - "You might like X because you rated Y highly and users who liked Y also enjoyed X"
   - Counterfactual explanations: "If you had rated Z differently, we'd recommend..."

5. **Production-Grade Infrastructure**
   - API endpoint (FastAPI)
   - Model versioning (MLflow)
   - Monitoring and logging
   - Automated retraining pipeline

---

## ğŸ“ Methodology Notes

### Evaluation Strategy

**Train-Test Split:**
- 80% training (80,000 ratings)
- 20% testing (20,000 ratings)
- Random split with seed=42 for reproducibility

**Cross-Validation:**
- Used 3-fold CV for hyperparameter tuning
- Ensures robust parameter selection
- Prevents overfitting to test set

**Metrics Choice:**
- **RMSE** (Root Mean Squared Error): Primary metric
  - Penalizes large errors more than small ones
  - Standard for rating prediction tasks
- **MAE** (Mean Absolute Error): Secondary metric
  - More interpretable (average prediction error)
  - Less sensitive to outliers
- **Ranking Metrics** (Precision, Recall, NDCG): For top-N evaluation
  - More relevant for real-world recommendation scenarios
  - Measure quality of ranked lists, not just ratings

### Why This Dataset?

**Advantages:**
- âœ… Industry-standard benchmark (comparable results)
- âœ… Manageable size (can iterate quickly)
- âœ… Dense enough for collaborative filtering (6.3% density)
- âœ… Includes demographics (enables cold start experiments)
- âœ… Multiple genres per movie (enables content-based filtering)

**Limitations:**
- âš ï¸ Small by modern standards (100K vs. millions of ratings)
- âš ï¸ Older dataset (1997-1998)
- âš ï¸ Genre categories are coarse (only 19 genres)
- âš ï¸ No temporal dynamics captured (all treated as static)

**Trade-offs acknowledged:** For a portfolio project demonstrating methodology and evaluation approaches, these limitations are acceptable. The focus is on showcasing technical skills, not achieving state-of-the-art results on production data.

---

## ğŸ™ Acknowledgments

### Dataset
- **GroupLens Research** at the University of Minnesota
- MovieLens dataset: [https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/)

### Libraries & Tools
- **Surprise Library** developers for excellent collaborative filtering tools
- **Streamlit** team for making deployment accessible
- **scikit-learn** community for robust ML infrastructure

### Inspiration
- Research papers on recommendation systems (SVD++, hybrid approaches)
- Netflix Prize competition (demonstrated importance of matrix factorization)
- Real-world ML practitioners who emphasize systematic evaluation over single models

---

## ğŸ“ Contact & Links

**Developer:** Harpinder Singh Chhabra  
**Email:** aekas142@gmail.com  
**GitHub:** [@HarpinderC](https://github.com/HarpinderC)  
**LinkedIn:** [Harpinder Singh Chhabra](https://linkedin.com/in/harpinder-singh-chhabra)  

**Project Repository:** [https://github.com/HarpinderC/movie-recommender-system](https://github.com/HarpinderC/ML-Recommenders/tree/102b1800d369674d729081ccfd7748e6ae3ba1a8/movie-recommendation-system)

---

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

---

## ğŸ¯ For Recruiters

**Why this project demonstrates value:**

1. **Systematic Approach**
   - Evaluated 12+ models across 4 paradigms
   - Documented what worked AND what didn't
   - Made data-driven decisions, not assumptions

2. **End-to-End Execution**
   - Data exploration â†’ Modeling â†’ Evaluation â†’ Deployment
   - Production-ready Streamlit application
   - Comprehensive documentation

3. **Technical Depth**
   - Implemented from scratch (not just library calls)
   - Understood trade-offs between approaches
   - Handled real-world challenges (cold start, sparsity)

4. **Communication Skills**
   - Clear documentation with personality
   - Visualizations that tell a story
   - Technical concepts explained accessibly

5. **Professional Quality**
   - Clean, modular code
   - Reproducible results
   - Version controlled
   - Deployment-ready

**Ready to discuss how these skills apply to your team's challenges!** ğŸš€

---

*Built with â¤ï¸ and a lot of â˜• by Harpinder Singh | December 2025*
